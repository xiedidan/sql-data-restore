#!/usr/bin/env python3
"""
SQLè§£æå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•

æ¯”è¾ƒä¸åŒè§£æç­–ç•¥çš„æ€§èƒ½å·®å¼‚
"""

import os
import sys
import time
import tempfile
import threading
from typing import Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

def create_test_sql_file(file_path: str, num_rows: int = 100000, table_name: str = "test_table"):
    """åˆ›å»ºæµ‹è¯•SQLæ–‡ä»¶"""
    print(f"åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {file_path} ({num_rows:,} è¡Œ)")
    
    with open(file_path, 'w', encoding='utf-8') as f:
        # å†™å…¥æ³¨é‡Š
        f.write(f"-- æµ‹è¯•SQLæ–‡ä»¶ï¼ŒåŒ…å« {num_rows:,} è¡Œæ•°æ®\n")
        f.write(f"-- è¡¨å: {table_name}\n")
        f.write("-- ç”Ÿæˆæ—¶é—´: " + time.strftime('%Y-%m-%d %H:%M:%S') + "\n\n")
        
        # å†™å…¥INSERTè¯­å¥
        for i in range(num_rows):
            if i % 10000 == 0:
                print(f"  å†™å…¥è¿›åº¦: {i:,}/{num_rows:,} ({i/num_rows*100:.1f}%)")
            
            row_id = i + 1
            name = f"User_{row_id}"
            email = f"user{row_id}@example.com"
            age = 20 + (i % 60)
            salary = 3000 + (i % 50) * 100
            department = f"Dept_{(i % 10) + 1}"
            created_date = f"2023-{(i%12)+1:02d}-{(i%28)+1:02d}"
            
            f.write(f"INSERT INTO {table_name} (id, name, email, age, salary, department, created_date) ")
            f.write(f"VALUES ({row_id}, '{name}', '{email}', {age}, {salary}, '{department}', '{created_date}');\n")
    
    file_size = os.path.getsize(file_path)
    print(f"æµ‹è¯•æ–‡ä»¶åˆ›å»ºå®Œæˆ: {file_size / (1024*1024):.2f} MB")
    return file_path

def benchmark_parser(parser_name: str, parser_instance, file_path: str, n_lines: int = 100) -> Dict:
    """åŸºå‡†æµ‹è¯•è§£æå™¨"""
    print(f"\n=== æµ‹è¯• {parser_name} ===")
    
    results = []
    
    # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•
    for i in range(3):
        start_time = time.time()
        
        try:
            if hasattr(parser_instance, 'extract_sample_data_fast'):
                result = parser_instance.extract_sample_data_fast(file_path, n_lines)
            elif hasattr(parser_instance, 'extract_sample_data_threaded'):
                result = parser_instance.extract_sample_data_threaded(file_path, n_lines)
            else:
                result = parser_instance.extract_sample_data(file_path, n_lines)
            
            end_time = time.time()
            parse_time = end_time - start_time
            
            results.append(parse_time)
            
            print(f"  æµ‹è¯• {i+1}: {parse_time:.3f}s")
            print(f"    è¡¨å: {result.get('table_name', 'N/A')}")
            print(f"    æ ·æœ¬è¡Œæ•°: {len(result.get('sample_data', []))}")
            print(f"    ä¼°è®¡æ€»è¡Œæ•°: {result.get('estimated_rows', 0):,}")
            
        except Exception as e:
            print(f"  æµ‹è¯• {i+1} å¤±è´¥: {str(e)}")
            results.append(float('inf'))
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    valid_results = [r for r in results if r != float('inf')]
    if valid_results:
        avg_time = sum(valid_results) / len(valid_results)
        min_time = min(valid_results)
        max_time = max(valid_results)
        
        print(f"  å¹³å‡æ—¶é—´: {avg_time:.3f}s")
        print(f"  æœ€ä½³æ—¶é—´: {min_time:.3f}s")
        print(f"  æœ€å·®æ—¶é—´: {max_time:.3f}s")
        
        return {
            'parser': parser_name,
            'avg_time': avg_time,
            'min_time': min_time,
            'max_time': max_time,
            'success_rate': len(valid_results) / len(results)
        }
    else:
        print(f"  æ‰€æœ‰æµ‹è¯•å¤±è´¥")
        return {
            'parser': parser_name,
            'avg_time': float('inf'),
            'min_time': float('inf'),
            'max_time': float('inf'),
            'success_rate': 0
        }

def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ SQLè§£æå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡ä»¶å¤§å°é…ç½®
    test_configs = [
        {'name': 'å°æ–‡ä»¶ (1ä¸‡è¡Œ)', 'rows': 10000},
        {'name': 'ä¸­ç­‰æ–‡ä»¶ (10ä¸‡è¡Œ)', 'rows': 100000},
        {'name': 'å¤§æ–‡ä»¶ (50ä¸‡è¡Œ)', 'rows': 500000},
    ]
    
    all_results = []
    
    for config in test_configs:
        print(f"\n{'='*20} {config['name']} {'='*20}")
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sql', delete=False) as temp_file:
            test_file = temp_file.name
        
        try:
            create_test_sql_file(test_file, config['rows'])
            file_size_mb = os.path.getsize(test_file) / (1024 * 1024)
            print(f"æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
            
            # åŸºç¡€é…ç½®
            base_config = {
                'migration': {'sample_lines': 100},
                'parser': {
                    'use_fast_parser': True,
                    'fast_parser_threshold': 0,  # æ€»æ˜¯ä½¿ç”¨å¿«é€Ÿè§£æå™¨
                    'chunk_size': 1024 * 1024,
                    'max_workers': 4
                }
            }
            
            test_results = []
            
            # 1. æµ‹è¯•ä¼ ç»Ÿè§£æå™¨
            try:
                from core.sql_parser import SQLFileParser
                traditional_parser = SQLFileParser(base_config)
                traditional_parser.use_fast_parser = False  # å¼ºåˆ¶ä½¿ç”¨ä¼ ç»Ÿè§£æ
                result = benchmark_parser("ä¼ ç»Ÿè§£æå™¨", traditional_parser, test_file, 100)
                test_results.append(result)
            except Exception as e:
                print(f"ä¼ ç»Ÿè§£æå™¨æµ‹è¯•å¤±è´¥: {e}")
            
            # 2. æµ‹è¯•å¿«é€Ÿè§£æå™¨
            try:
                from core.fast_sql_parser import FastSQLParser
                fast_parser = FastSQLParser(base_config)
                result = benchmark_parser("å¿«é€Ÿè§£æå™¨", fast_parser, test_file, 100)
                test_results.append(result)
            except Exception as e:
                print(f"å¿«é€Ÿè§£æå™¨æµ‹è¯•å¤±è´¥: {e}")
            
            # 3. æµ‹è¯•å¤šçº¿ç¨‹è§£æå™¨ï¼ˆä»…å¤§æ–‡ä»¶ï¼‰
            if config['rows'] >= 100000:
                try:
                    from core.fast_sql_parser import ThreadedSQLParser
                    threaded_parser = ThreadedSQLParser(base_config)
                    result = benchmark_parser("å¤šçº¿ç¨‹è§£æå™¨", threaded_parser, test_file, 100)
                    test_results.append(result)
                except Exception as e:
                    print(f"å¤šçº¿ç¨‹è§£æå™¨æµ‹è¯•å¤±è´¥: {e}")
            
            # ä¿å­˜ç»“æœ
            for result in test_results:
                result['file_config'] = config['name']
                result['file_size_mb'] = file_size_mb
                all_results.append(result)
                
        finally:
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            if os.path.exists(test_file):
                os.unlink(test_file)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    print_performance_report(all_results)

def print_performance_report(results: List[Dict]):
    """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
    print(f"\n{'='*60}")
    print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    print(f"{'='*60}")
    
    if not results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    # æŒ‰æ–‡ä»¶é…ç½®åˆ†ç»„
    configs = {}
    for result in results:
        config_name = result['file_config']
        if config_name not in configs:
            configs[config_name] = []
        configs[config_name].append(result)
    
    for config_name, config_results in configs.items():
        print(f"\n{config_name}")
        print("-" * 40)
        
        # æŒ‰å¹³å‡æ—¶é—´æ’åº
        config_results.sort(key=lambda x: x['avg_time'])
        
        baseline_time = None
        for i, result in enumerate(config_results):
            if result['success_rate'] > 0:
                if baseline_time is None:
                    baseline_time = result['avg_time']
                    speedup = 1.0
                else:
                    speedup = baseline_time / result['avg_time']
                
                print(f"{i+1}. {result['parser']:<15} | "
                      f"å¹³å‡: {result['avg_time']:.3f}s | "
                      f"æœ€ä½³: {result['min_time']:.3f}s | "
                      f"æˆåŠŸç‡: {result['success_rate']*100:.0f}% | "
                      f"åŠ é€Ÿæ¯”: {speedup:.1f}x")
            else:
                print(f"{i+1}. {result['parser']:<15} | æµ‹è¯•å¤±è´¥")
    
    # æ€»ç»“å»ºè®®
    print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    
    # æ‰¾å‡ºæœ€ä½³è§£æå™¨
    successful_results = [r for r in results if r['success_rate'] > 0]
    if successful_results:
        best_result = min(successful_results, key=lambda x: x['avg_time'])
        print(f"âœ… æ¨èä½¿ç”¨: {best_result['parser']}")
        print(f"   å¹³å‡è§£ææ—¶é—´: {best_result['avg_time']:.3f}s")
        
        # è®¡ç®—æ€§èƒ½æå‡
        traditional_results = [r for r in successful_results if 'ä¼ ç»Ÿ' in r['parser']]
        if traditional_results:
            traditional_time = traditional_results[0]['avg_time']
            improvement = ((traditional_time - best_result['avg_time']) / traditional_time) * 100
            print(f"   ç›¸æ¯”ä¼ ç»Ÿè§£æå™¨æå‡: {improvement:.1f}%")
    
    # æ–‡ä»¶å¤§å°å»ºè®®
    print(f"\nğŸ“ æ–‡ä»¶å¤§å°å»ºè®®:")
    print(f"â€¢ å°æ–‡ä»¶ (<10MB): ä½¿ç”¨å¿«é€Ÿè§£æå™¨")
    print(f"â€¢ ä¸­ç­‰æ–‡ä»¶ (10-100MB): ä½¿ç”¨å¿«é€Ÿè§£æå™¨ + å†…å­˜æ˜ å°„")
    print(f"â€¢ å¤§æ–‡ä»¶ (>100MB): ä½¿ç”¨å¤šçº¿ç¨‹è§£æå™¨")

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print(f"\nğŸ§  å†…å­˜ä½¿ç”¨æµ‹è¯•")
    print("-" * 40)
    
    try:
        import psutil
        process = psutil.Process()
        
        # æµ‹è¯•å‰å†…å­˜
        memory_before = process.memory_info().rss / (1024 * 1024)
        print(f"æµ‹è¯•å‰å†…å­˜: {memory_before:.1f} MB")
        
        # åˆ›å»ºå¤§æ–‡ä»¶å¹¶è§£æ
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sql', delete=False) as temp_file:
            test_file = temp_file.name
        
        try:
            create_test_sql_file(test_file, 100000)
            
            from core.fast_sql_parser import FastSQLParser
            parser = FastSQLParser({'migration': {'sample_lines': 100}})
            
            result = parser.extract_sample_data_fast(test_file, 100)
            
            # æµ‹è¯•åå†…å­˜
            memory_after = process.memory_info().rss / (1024 * 1024)
            memory_used = memory_after - memory_before
            
            print(f"æµ‹è¯•åå†…å­˜: {memory_after:.1f} MB")
            print(f"å†…å­˜å¢é•¿: {memory_used:.1f} MB")
            print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(test_file) / (1024*1024):.1f} MB")
            print(f"å†…å­˜æ•ˆç‡: {memory_used / (os.path.getsize(test_file) / (1024*1024)):.2f}")
            
        finally:
            if os.path.exists(test_file):
                os.unlink(test_file)
                
    except ImportError:
        print("âŒ psutilä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        print("   å®‰è£…æ–¹æ³•: pip install psutil")

if __name__ == "__main__":
    try:
        run_performance_benchmark()
        test_memory_usage()
        
        print(f"\nâœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        print(f"ğŸ’¡ åŸºäºæµ‹è¯•ç»“æœï¼Œå»ºè®®åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®åˆé€‚çš„è§£æå™¨å‚æ•°")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()